input {
    
    # config for file input
    file {
        mode => tail
        path => "/var/log/syslog"
        start_position => end
        id => "file"
    }

    # config for Splunk Forwarder
#    tcp {
#        host => "127.0.0.1"
#        port => 20001
#        mode => "server"
#        id => "splunk_fwd"
#    }

    # config for Elastic Beats input
#    beats {
#        host => "127.0.0.1"
#        port => 20001
#        id => "elastic_beat_"
#    }
}

filter{
    # parse log. MUST parse out a timestamp
    # %{PATTERN:variable_name}
    # SYSLOGBASE parses a lot of fields. you can test with
    # https://grokdebug.herokuapp.com/
    grok {
        match => { "message" => ["%{SYSLOGBASE:syslog} %{GREEDYDATA:rest}"] }
#       match => { "message" => ["%{GREEDYDATA:rest}"] }
    }

    # prevent recursive logging
    if "logstash" in [program] { drop { } }

    # skip lines that do not match the filter
    if "_grokparsefailure" in [tags] { drop { } }

    # validate timestamp and convert to ISO8601
    # change first parameter to your timestamp variable.
    # change 2..n params to match your incoming timestamp(s) 
    date {
        match => [ "timestamp", "MMM dd HH:mm:ss", "MMM  d HH:mm:ss", "ISO8601" ]
    }

    # convert the timestamp into unix epoch (ms)
    ruby {  
	    code => "event.set('ts_epoch', event.get('@timestamp').to_i * 1000)"
    }

    # add msg field and labeled subfields
    mutate {
        add_field => {
            "[msg][tag]" => "%{host}"
            "[msg][eventId]" => "%{ts_epoch}"
            "[msg][data]" => "%{message}"
        }
    }

    # turn the msg built in mutate into json
    # must install the json_encode plugin
    # bin/logstash-plugin install logstash-filter-json_encode
    json_encode {
        source => "msg"
        target => "data"
    }
    
    # add/update fields per line read
    aggregate {
        task_id => "%{host}"
        code => "
            # initialize fields	
            map['size'] ||= 0 
            map['count'] ||= 0
            map['groupdata'] ||= []
            # put data into groupdata (index is line #)
            map['groupdata'] << event.get('data')
            # keep track of size and count. add 20 as offset (change if needed)
            map['size'] += (event.get('data').size + 20)
            map['count'] += 1
            # chunk output to 2-3MB
            # fiddle with these values as needed
            map_meta.timeout = 0 if map['count'] == 999999 or (map['size'] + 400) > 999999
            event.cancel()
        "
        push_previous_map_as_event => true
        push_map_as_event_on_timeout => true
        timeout => 10
    }
}

# configure output
output {
    stdout { codec => rubydebug }

    # send data to http 
    http {
        # url to send output to (InsightFinder)
        url => "http://127.0.0.1:8080/api/v1/customprojectrawdata"
        http_method => post
        format => form

        # need to update userName, licenseKey, and projectName
        mapping => {
            "userName" => "user"
            "licenseKey" => "a333bd39662ad1edf64e99e97e4b776109827234"
            "projectName" => "splunkForwardingStream"
            "agentType" => "LogStreaming"
            "metricData" => "[%{groupdata}]"
        }
    }
}
